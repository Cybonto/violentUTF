name: Nightly Deep Testing

on:
  schedule:
    - cron: '0 3 * * *'  # Daily at 3 AM UTC
  workflow_dispatch:
    inputs:
      test_scope:
        description: 'Test scope to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - security
          - performance
          - compatibility
          - stress

permissions:
  contents: read
  security-events: write
  issues: write

jobs:
  deep-security-analysis:
    name: Deep Security Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 60
    if: github.event.inputs.test_scope == 'all' || github.event.inputs.test_scope == 'security'

    steps:
      - name: Checkout code
        uses: actions/checkout@eef61447b9ff4aafe5dcd4e0bbf5d482be7e7871 # v5.0.0
        with:
          fetch-depth: 0  # Full history for analysis

      - name: Set up Python
        uses: actions/setup-python@0b93645e9fea7318ecaed2b359559ac225c90a2b # v5.6.0
        with:
          python-version: '3.11'

      - name: Install security tools
        run: |
          python -m pip install --upgrade pip
          pip install bandit safety pip-audit semgrep pylint
          pip install truffleHog3 detect-secrets gitpython

      - name: Secret scanning
        run: |
          echo "## Secret Scanning Results" > security-report.md
          echo "" >> security-report.md

          # TruffleHog scan
          trufflehog3 --no-history --format json -o trufflehog-report.json || true

          # detect-secrets scan
          detect-secrets scan --all-files > secrets-baseline.json || true
          detect-secrets audit secrets-baseline.json || true

      - name: Comprehensive vulnerability scan
        run: |
          echo "" >> security-report.md
          echo "## Vulnerability Scan" >> security-report.md
          echo "" >> security-report.md

          # Run safety with full database
          safety check --full-report --output json > safety-full-report.json || true

          # Run pip-audit with OSV database
          pip-audit --desc --format json --vulnerability-service osv > pip-audit-osv.json || true

      - name: SAST with multiple tools
        run: |
          echo "" >> security-report.md
          echo "## Static Analysis Results" >> security-report.md
          echo "" >> security-report.md

          # Semgrep with multiple rulesets
          semgrep --config=auto --config=p/security-audit --config=p/owasp-top-ten \
                  --json -o semgrep-full-report.json || true

          # Bandit with all plugins
          bandit -r . -ll -i -f json -o bandit-full-report.json 2>/dev/null || true

      - name: License compliance check
        run: |
          pip install pip-licenses
          echo "" >> security-report.md
          echo "## License Compliance" >> security-report.md
          echo "" >> security-report.md
          pip-licenses --with-urls --format=json > licenses.json
          python -c "import json; f = open('licenses.json'); licenses = json.load(f); f.close(); problematic = [l for l in licenses if l.get('License', '').upper() in ['GPL', 'AGPL', 'UNKNOWN']]; print(f'Found {len(problematic)} potentially problematic licenses') if problematic else None; [print(f\"- {l.get('Name')}: {l.get('License')}\") for l in problematic[:10]]"

      - name: Upload security reports
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: nightly-security-reports
          path: |
            security-report.md
            *-report.json
            secrets-baseline.json

  compatibility-testing:
    name: Extended Compatibility Testing
    runs-on: ${{ matrix.os }}
    timeout-minutes: 90
    if: github.event.inputs.test_scope == 'all' || github.event.inputs.test_scope == 'compatibility'
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-20.04, ubuntu-22.04, windows-2019, windows-2022, macos-12, macos-13]
        python-version: ['3.10', '3.11', '3.12', '3.13-dev']
        exclude:
          # Exclude Python 3.13-dev on older OS versions
          - os: ubuntu-20.04
            python-version: '3.13-dev'
          - os: windows-2019
            python-version: '3.13-dev'
          - os: macos-12
            python-version: '3.13-dev'

    steps:
      - name: Checkout code
        uses: actions/checkout@eef61447b9ff4aafe5dcd4e0bbf5d482be7e7871 # v5.0.0

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@0b93645e9fea7318ecaed2b359559ac225c90a2b # v5.6.0
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
        continue-on-error: ${{ matrix.python-version == '3.13-dev' }}

      - name: Test installation process
        run: |
          python -m pip install --upgrade pip setuptools wheel
          # Test clean installation
          pip install -e . || pip install . || true

      - name: Test with minimal dependencies
        run: |
          # Create minimal requirements
          grep -E '^(fastapi|streamlit|pyrit|garak)' requirements.txt > minimal-requirements.txt || true
          pip install -r minimal-requirements.txt || true

      - name: Test with latest dependencies
        run: |
          # Upgrade all dependencies to latest
          pip install --upgrade -r requirements.txt || true

      - name: Run compatibility tests
        run: |
          cat > test_imports.py << 'EOF'
          import sys
          print(f'Python: {sys.version}')
          print(f'Platform: {sys.platform}')

          # Test critical imports
          try:
              import fastapi
              print('FastAPI: OK')
          except ImportError as e:
              print(f'FastAPI: FAILED - {e}')

          try:
              import streamlit
              print('Streamlit: OK')
          except ImportError as e:
              print(f'Streamlit: FAILED - {e}')

          try:
              from pyrit.orchestrator import PromptSendingOrchestrator
              print('PyRIT: OK')
          except ImportError as e:
              print(f'PyRIT: FAILED - {e}')
          EOF
          python test_imports.py

  performance-stress-testing:
    name: Performance and Stress Testing
    runs-on: ubuntu-latest
    timeout-minutes: 120
    if: github.event.inputs.test_scope == 'all' || github.event.inputs.test_scope == 'performance' || github.event.inputs.test_scope == 'stress'

    steps:
      - name: Checkout code
        uses: actions/checkout@eef61447b9ff4aafe5dcd4e0bbf5d482be7e7871 # v5.0.0

      - name: Set up Python
        uses: actions/setup-python@0b93645e9fea7318ecaed2b359559ac225c90a2b # v5.6.0
        with:
          python-version: '3.11'

      - name: Install performance tools
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt || true
          pip install locust pytest-benchmark memory_profiler py-spy
          pip install httpx aiohttp uvloop

      - name: Start services for load testing
        run: |
          docker compose up -d
          # Wait for services
          sleep 60

      - name: Run load tests
        run: |
          # Create locust test file
          cat > locustfile.py << 'EOF'
          from locust import HttpUser, task, between

          class ViolentUTFUser(HttpUser):
              wait_time = between(1, 3)

              @task
              def health_check(self):
                  self.client.get("/health")

              @task
              def list_generators(self):
                  self.client.get("/api/v1/generators")

              @task
              def list_datasets(self):
                  self.client.get("/api/v1/datasets")
          EOF

          # Run load test
          locust -f locustfile.py \
                 --headless \
                 --users 100 \
                 --spawn-rate 10 \
                 --run-time 5m \
                 --host http://localhost:9080 \
                 --html performance-report.html || true

      - name: Memory profiling
        run: |
          # Profile memory usage
          mprof run python -m pytest tests/unit/ || true
          mprof plot -o memory-usage.png || true

      - name: CPU profiling
        run: |
          # Profile CPU usage
          py-spy record -d 30 -o cpu-profile.svg -- python -m pytest tests/unit/ || true

      - name: Stress test with edge cases
        run: |
          # Test with large payloads
          cat > stress_test.py << 'EOF'
          import httpx
          import json
          import asyncio

          async def stress_test():
              async with httpx.AsyncClient(timeout=30.0) as client:
                  # Test large dataset
                  large_data = {'prompts': ['test' * 1000 for _ in range(1000)]}
                  try:
                      response = await client.post(
                          'http://localhost:9080/api/v1/datasets',
                          json=large_data
                      )
                      print(f'Large dataset test: {response.status_code}')
                  except Exception as e:
                      print(f'Large dataset test failed: {e}')

                  # Test concurrent requests
                  tasks = []
                  for i in range(100):
                      task = client.get('http://localhost:9080/api/v1/health')
                      tasks.append(task)

                  responses = await asyncio.gather(*tasks, return_exceptions=True)
                  success_count = sum(1 for r in responses if not isinstance(r, Exception))
                  print(f'Concurrent requests: {success_count}/100 successful')

          asyncio.run(stress_test())
          EOF
          python stress_test.py

      - name: Upload performance results
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: performance-results
          path: |
            performance-report.html
            memory-usage.png
            cpu-profile.svg

  framework-integration-testing:
    name: AI Framework Deep Testing
    runs-on: ubuntu-latest
    timeout-minutes: 90
    if: false  # Disabled for online CI - can be enabled for local testing

    steps:
      - name: Checkout code
        uses: actions/checkout@eef61447b9ff4aafe5dcd4e0bbf5d482be7e7871 # v5.0.0

      - name: Set up Python
        uses: actions/setup-python@0b93645e9fea7318ecaed2b359559ac225c90a2b # v5.6.0
        with:
          python-version: '3.11'

      - name: Install frameworks
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt || true

      - name: Test PyRIT components
        run: |
          echo "## PyRIT Component Testing" > framework-report.md
          echo "" >> framework-report.md

          # Test all PyRIT orchestrators
          cat > test_pyrit.py << 'EOF'
          from pyrit.orchestrator import (
              PromptSendingOrchestrator,
              # Add other orchestrators as they become available
          )
          print('PyRIT orchestrators imported successfully')

          # Test all scorers
          from pyrit.score import (
              SubStringScorer,
              SelfAskLikertScorer,
              # Add other scorers
          )
          print('PyRIT scorers imported successfully')

          # Test memory systems
          from pyrit.memory import CentralMemory, MemoryInterface
          print('PyRIT memory systems imported successfully')
          EOF
          python test_pyrit.py

      - name: Test Garak components
        run: |
          echo "" >> framework-report.md
          echo "## Garak Component Testing" >> framework-report.md
          echo "" >> framework-report.md

          # List all Garak probes
          garak --list-probes > garak-probes.txt || true

          # List all Garak detectors
          garak --list-detectors > garak-detectors.txt || true

          # Test basic Garak functionality
          garak --model-type test --probes encoding --detectors always.Pass || true

      - name: Integration scenario testing
        run: |
          # Test realistic attack scenarios
          python tests/scenarios/test_jailbreak_scenario.py || true
          python tests/scenarios/test_prompt_injection_scenario.py || true
          python tests/scenarios/test_bias_detection_scenario.py || true

      - name: Upload framework test results
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: framework-test-results
          path: |
            framework-report.md
            garak-*.txt

  nightly-report:
    name: Generate Nightly Report
    needs: [deep-security-analysis, compatibility-testing, performance-stress-testing]  # framework-integration-testing disabled for CI
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@d3f86a106a0bac45b974a628896c90dbdf5c8093 # v4.3.0
        with:
          path: nightly-artifacts
          merge-multiple: true

      - name: Generate consolidated report
        run: |
          echo "# Nightly Testing Report - $(date +'%Y-%m-%d')" > nightly-report.md
          echo "" >> nightly-report.md

          echo "## Test Summary" >> nightly-report.md
          echo "- Deep Security Analysis: ${{ needs.deep-security-analysis.result }}" >> nightly-report.md
          echo "- Compatibility Testing: ${{ needs.compatibility-testing.result }}" >> nightly-report.md
          echo "- Performance Testing: ${{ needs.performance-stress-testing.result }}" >> nightly-report.md
          echo "- Framework Testing: skipped (disabled for CI)" >> nightly-report.md
          echo "" >> nightly-report.md

          # Aggregate findings
          echo "## Key Findings" >> nightly-report.md
          echo "" >> nightly-report.md

          # Check for security issues
          if [ -f nightly-artifacts/nightly-security-reports/bandit-full-report.json ]; then
            echo "### Security Issues" >> nightly-report.md
            python -c "import json; f = open('nightly-artifacts/nightly-security-reports/bandit-full-report.json'); data = json.load(f); f.close(); issues = data.get('results', []); high_severity = [i for i in issues if i.get('issue_severity') == 'HIGH']; print(f'Found {len(high_severity)} high severity security issues') if high_severity else None" >> nightly-report.md
          fi

      - name: Create issue if tests failed
        if: failure()
        uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea # v7.0.1
        with:
          script: |
            const date = new Date().toISOString().split('T')[0];
            const issue = await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Nightly Test Failures - ${date}`,
              body: `The nightly test suite has detected failures. Please review the [workflow run](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}) for details.`,
              labels: ['nightly-failure', 'automated']
            });
            console.log(`Created issue #${issue.data.number}`);

      - name: Upload nightly report
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: nightly-report
          path: nightly-report.md
          retention-days: 30
