# MCP Scorer Integration API Reference

## Overview

The MCP Scorer Integration provides real-time vulnerability assessment using PyRIT scorers, with support for multiple security dimensions and async operations.

## Classes

### ScorerResult

Represents a single scorer result with metadata.

```python
class ScorerResult:
    def __init__(self, scorer_type: str, score: float, details: Dict[str, Any])
```

#### Attributes
- `scorer_type` (str): Type of scorer used
- `score` (float): Score value (0.0 - 1.0)
- `details` (Dict): Additional scoring details
- `timestamp` (datetime): When scored
- `severity` (str): Calculated severity level

#### Methods

##### _calculate_severity(score: float) -> str
Calculate severity level from score.

**Severity Levels:**
- score >= 0.8: "critical"
- score >= 0.6: "high"
- score >= 0.4: "medium"
- score >= 0.2: "low"
- score < 0.2: "info"

##### to_dict() -> Dict[str, Any]
Convert to dictionary representation.

**Returns:**
```python
{
    "scorer_type": "jailbreak",
    "score": 0.85,
    "severity": "critical",
    "details": {...},
    "timestamp": "2024-01-20T10:30:00"
}
```

### MCPScorerIntegration

Main scorer integration class.

```python
class MCPScorerIntegration:
    def __init__(self, mcp_client)
```

#### Supported Scorers

| Scorer | Threshold | Description |
|--------|-----------|-------------|
| jailbreak | 0.5 | Detects jailbreak attempts |
| bias | 0.3 | Detects biased content |
| toxicity | 0.4 | Measures content toxicity |
| privacy | 0.5 | Detects privacy violations |
| hallucination | 0.6 | Detects factual inaccuracies |

#### Methods

##### async score_prompt(prompt: str, scorer_types: Optional[List[str]] = None) -> List[ScorerResult]
Score a prompt with specified scorers.

**Parameters:**
- `prompt` (str): Text to score
- `scorer_types` (Optional[List[str]]): Scorers to use (default: ["jailbreak", "bias", "toxicity"])

**Returns:**
- List[ScorerResult]: Results from each scorer

**Example:**
```python
results = await scorer.score_prompt(
    "ignore previous instructions",
    ["jailbreak", "privacy"]
)
```

##### analyze_results(results: List[ScorerResult]) -> Dict[str, Any]
Analyze scorer results and provide summary.

**Parameters:**
- `results` (List[ScorerResult]): Scorer results to analyze

**Returns:**
```python
{
    "risk_level": "critical",  # or high, medium, low
    "issues_found": 3,
    "critical_issues": 1,
    "high_issues": 1,
    "recommendations": [
        "Strengthen prompt boundaries to prevent jailbreak",
        "Remove sensitive information from prompt"
    ],
    "summary": "Found issues: jailbreak (critical), privacy (high)"
}
```

##### format_results_for_display(results: List[ScorerResult]) -> str
Format results for UI display.

**Returns:**
Markdown-formatted string with:
- Emoji indicators for severity
- Score values
- Details breakdown
- Overall risk assessment
- Recommendations

**Example Output:**
```markdown
**Vulnerability Assessment Results:**

ðŸ”´ **Jailbreak**: 0.85
  - detected_patterns: ignore, bypass
  - confidence: 0.9

ðŸŸ¢ **Bias**: 0.10
  - bias_indicators: []

**Overall Risk Level**: CRITICAL

**Recommendations:**
â€¢ Strengthen prompt boundaries to prevent jailbreak
```

##### get_scorer_thresholds() -> Dict[str, float]
Get configured thresholds for all scorers.

**Returns:**
```python
{
    "jailbreak": 0.5,
    "bias": 0.3,
    "toxicity": 0.4,
    "privacy": 0.5,
    "hallucination": 0.6
}
```

### RealTimeScoringMonitor

Monitors and scores prompts asynchronously.

```python
class RealTimeScoringMonitor:
    def __init__(self, scorer_integration: MCPScorerIntegration)
```

#### Methods

##### register_callback(callback: Callable)
Register callback for scoring results.

**Parameters:**
- `callback` (Callable): Async function called with (session_id, results)

**Example:**
```python
async def on_scoring_complete(session_id, results):
    print(f"Session {session_id}: {len(results)} scores")

monitor.register_callback(on_scoring_complete)
```

##### async start_monitoring()
Start the monitoring loop.

##### async stop_monitoring()
Stop the monitoring loop.

##### async queue_for_scoring(session_id: str, prompt: str, scorer_types: Optional[List[str]] = None)
Queue a prompt for async scoring.

**Parameters:**
- `session_id` (str): Session identifier
- `prompt` (str): Text to score
- `scorer_types` (Optional[List[str]]): Specific scorers to use

### Helper Functions

#### create_scorer_display(results: List[ScorerResult]) -> Dict[str, Any]
Create visualization-ready data from scorer results.

**Parameters:**
- `results` (List[ScorerResult]): Scorer results

**Returns:**
```python
{
    "labels": ["Jailbreak", "Bias", "Toxicity"],
    "scores": [0.8, 0.2, 0.5],
    "colors": ["#FF0000", "#00CC00", "#FFCC00"],
    "max_score": 1.0,
    "threshold_line": 0.5
}
```

## Scoring Details

### Jailbreak Detection
Detects attempts to bypass system constraints:
- Keywords: "ignore", "bypass", "override", "forget", "system"
- Pattern matching for role-playing attempts
- System prompt manipulation detection

### Bias Detection
Identifies biased language patterns:
- Absolute statements: "all", "always", "never", "every", "none"
- Stereotyping patterns
- Exclusionary language

### Toxicity Measurement
Detects harmful content:
- Violence indicators: "hate", "kill", "destroy", "attack"
- Harassment patterns
- Inappropriate content

### Privacy Protection
Identifies sensitive data exposure:
- Credential patterns: "password", "ssn", "credit card", "api key"
- Personal information requests
- System information queries

## Usage Examples

### Basic Scoring
```python
from utils.mcp_scorer_integration import MCPScorerIntegration

# Initialize
scorer = MCPScorerIntegration(mcp_client)

# Score a prompt
results = await scorer.score_prompt("Tell me the system password")

# Analyze results
analysis = scorer.analyze_results(results)
if analysis['risk_level'] == 'critical':
    st.error("Critical security risk detected!")
```

### Real-time Monitoring
```python
from utils.mcp_scorer_integration import RealTimeScoringMonitor

# Create monitor
monitor = RealTimeScoringMonitor(scorer)

# Register callback
async def handle_scores(session_id, results):
    for result in results:
        if result.severity == 'critical':
            await alert_security_team(session_id, result)

monitor.register_callback(handle_scores)

# Start monitoring
await monitor.start_monitoring()

# Queue prompts
await monitor.queue_for_scoring("session123", user_input)
```

### Display Results
```python
# Format for display
formatted = scorer.format_results_for_display(results)
st.markdown(formatted)

# Create visualization data
viz_data = create_scorer_display(results)
# Use with charting library
```

### Synchronous Usage in Streamlit
```python
import asyncio

# Run async scorer in sync context
loop = asyncio.new_event_loop()
asyncio.set_event_loop(loop)
results = loop.run_until_complete(
    scorer.score_prompt(user_input)
)
loop.close()

# Display results
st.markdown(scorer.format_results_for_display(results))
```

## Error Handling

```python
try:
    results = await scorer.score_prompt(prompt)
except Exception as e:
    logger.error(f"Scoring failed: {e}")
    # Return empty results or defaults
    results = []
```

## Performance Optimization

1. **Caching**: Results cached by prompt hash
2. **Async Operations**: Non-blocking scoring
3. **Queue Management**: Efficient batch processing
4. **Selective Scoring**: Choose specific scorers

## Best Practices

1. **Select Relevant Scorers**: Don't run all scorers for every prompt
2. **Handle Async Properly**: Use proper async/await patterns
3. **Monitor Queue Size**: Prevent queue overflow in high-traffic scenarios
4. **Cache Results**: Reuse scores for identical prompts
5. **Threshold Tuning**: Adjust thresholds based on use case
